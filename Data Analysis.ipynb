{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c870ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check for precedents\n",
    "import json\n",
    "\n",
    "with open('jsonformatter.txt', 'r', encoding='utf-8') as f:\n",
    "    with open(\"Precedents output.txt\", 'w', encoding='utf-8') as out:\n",
    "        data = json.load(f)\n",
    "\n",
    "        for case in data:\n",
    "            out.write('Case id : ' + str(case['id']) + '\\n')\n",
    "            annotation = case['annotations'][0]\n",
    "            for key in annotation['result']:\n",
    "                if key['value']['labels'][0] == 'STA':\n",
    "                    out.write('STA : ' + key['value']['text'] + '\\n\\n')\n",
    "            out.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check for precedents relied\n",
    "import json\n",
    "\n",
    "with open('jsonformatter.txt', 'r', encoding='utf-8') as f:\n",
    "    with open(\"Precedents relied output.txt\", 'w', encoding='utf-8') as out:\n",
    "        data = json.load(f)\n",
    "\n",
    "        for case in data:\n",
    "            out.write('Id : ' + str(case['id']) + '\\n')\n",
    "            annotation = case['annotations'][0]\n",
    "            for key in annotation['result']:\n",
    "                if key['value']['labels'][0] == 'PRE_RELIED':\n",
    "                    out.write('PRE_RELIED : ' + key['value']['text'] + '\\n\\n')\n",
    "\n",
    "            out.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea61b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check for precedents not relied\n",
    "import json\n",
    "\n",
    "with open('jsonformatter.txt', 'r', encoding='utf-8') as f:\n",
    "    with open(\"Precedents not relied output.txt\", 'w', encoding='utf-8') as out:\n",
    "        data = json.load(f)\n",
    "\n",
    "        for case in data:\n",
    "            out.write('Id : ' + str(case['id']) + '\\n')\n",
    "            annotation = case['annotations'][0]\n",
    "            for key in annotation['result']:\n",
    "                if key['value']['labels'][0] == 'PRE_NOT_RELIED':\n",
    "                    out.write('PRE_NOT_RELIED : ' + key['value']['text'] + '\\n\\n')\n",
    "\n",
    "            out.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42899f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To seperate the precedents on the keyword \"vs\" or \"v.\"\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "directories = ['Writ Petition', 'Civil Appeal']\n",
    "for directory in directories:\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        mytree = ET.ElementTree(file= f)\n",
    "        myroot = mytree.getroot()\n",
    "\n",
    "        out_dir = directory+' precedents output'\n",
    "        out_file = os.path.join(out_dir, filename.replace(\".xml\", \" output.txt\"))\n",
    "        with open( out_file, 'w') as out:\n",
    "            for i in myroot:\n",
    "                if(i.tag=='JudgmentText'):\n",
    "                    for legis in i:\n",
    "                        # out.write(\"Inside Judgement Text : \" + legis.tag + '\\n')\n",
    "                        if(legis.tag=='I'):\n",
    "                            for para in legis:\n",
    "                                if para.text is None:\n",
    "                                    continue\n",
    "                                s1 = para.text.lower()\n",
    "                                s2 = para.text\n",
    "                                if ' vs' in s1 or ' v. ' in s2 or ' v ' in s2:\n",
    "                                    out.write(\"Precedent : \" + para.text + '\\n\\n')\n",
    "\n",
    "                        elif(legis.tag=='P'):\n",
    "\n",
    "                            if legis.text is None:\n",
    "                                continue\n",
    "\n",
    "                            s1 = legis.text.lower()\n",
    "                            s2 = legis.text\n",
    "                            if ' vs' in s1 or ' v. ' in s2 or ' v ' in s2:\n",
    "                                out.write(\"Precedent : \" + legis.text + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9769d968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "#To classify precedents\n",
    "! pip install nltk\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "directories = ['Writ Petition', 'Civil Appeal']\n",
    "for directory in directories:\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        mytree = ET.ElementTree(file= f)\n",
    "        myroot = mytree.getroot()\n",
    "\n",
    "        out_dir = directory+' precedents classified output'\n",
    "        out_file = os.path.join(out_dir, filename.replace(\".xml\", \" output.txt\"))\n",
    "        with open( out_file, 'w') as out:\n",
    "            for i in myroot:\n",
    "                if(i.tag=='JudgmentText'):\n",
    "                    for legis in i:\n",
    "                        if(legis.tag=='I'):\n",
    "                            for para in legis:\n",
    "                                if para.text is None:\n",
    "                                    continue\n",
    "                                s1 = para.text.lower()\n",
    "                                s2 = para.text\n",
    "                                if ' vs' in s1 or ' v. ' in s2 or ' v ' in s2:\n",
    "                                    if 'overruled' in s1:\n",
    "                                        out.write(\"Precedent Overruled : \")\n",
    "                                    elif 'distinguish' in s1:\n",
    "                                        out.write(\"Precedent Distinguished : \")\n",
    "                                    elif sia.polarity_scores(s2)[\"compound\"] > 0:\n",
    "                                        out.write(\"Precedent Relied : \")\n",
    "                                    else:\n",
    "                                        out.write(\"Precedent Referred : \")\n",
    "                                    out.write(s2 + '\\n\\n')\n",
    "\n",
    "                        elif(legis.tag=='P'):\n",
    "\n",
    "                            if legis.text is None:\n",
    "                                continue\n",
    "\n",
    "                            s1 = legis.text.lower()\n",
    "                            s2 = legis.text\n",
    "                            if ' vs' in s1 or ' v. ' in s2 or ' v ' in s2:\n",
    "                                if 'overruled' in s1:\n",
    "                                    out.write(\"Precedent Overruled : \")\n",
    "                                elif 'distinguish' in s1:\n",
    "                                    out.write(\"Precedent Distinguished : \")\n",
    "                                elif sia.polarity_scores(s2)[\"compound\"] > 0:\n",
    "                                    out.write(\"Precedent Relied : \")\n",
    "                                else:\n",
    "                                    out.write(\"Precedent Referred : \")\n",
    "                                out.write(s2 + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33715c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\andre/nltk_data'\n",
      "    - 'c:\\\\Users\\\\andre\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\andre\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\andre\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\andre\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\andre/nltk_data'\n",
      "    - 'c:\\\\Users\\\\andre\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\andre\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\andre\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\andre\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.data import find\n",
    "\n",
    "try:\n",
    "    print(find('tokenizers/punkt/english/'))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    print(find('tokenizers/punkt_tab/english/'))\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e0379aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a sentence.', 'This is another one.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Forzar a que el tokenizador de oraciones sea el estándar 'punkt'\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "\n",
    "punkt_param = PunktParameters()\n",
    "tokenizer = PunktSentenceTokenizer(punkt_param)\n",
    "\n",
    "# Luego en lugar de sent_tokenize(text) usa:\n",
    "# tokenizer.tokenize(text)\n",
    "\n",
    "# Ejemplo:\n",
    "text = \"This is a sentence. This is another one.\"\n",
    "\n",
    "sentences = tokenizer.tokenize(text)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f9224c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Sentence wise classification of precedents\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Descargas necesarias\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "\n",
    "# Inicializar tokenizer manualmente para evitar búsqueda de 'punkt_tab'\n",
    "punkt_param = PunktParameters()\n",
    "tokenizer = PunktSentenceTokenizer(punkt_param)\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "directories = ['Writ Petition', 'Civil Appeal']\n",
    "\n",
    "for directory in directories:\n",
    "    out_dir = directory + ' precedents classified output'\n",
    "    os.makedirs(out_dir, exist_ok=True)  # Crear carpeta si no existe\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "\n",
    "        mytree = ET.parse(file_path)\n",
    "        myroot = mytree.getroot()\n",
    "\n",
    "        out_file = os.path.join(out_dir, filename.replace(\".xml\", \" output.txt\"))\n",
    "        with open(out_file, 'w', encoding='utf-8') as out:\n",
    "            for i in myroot:\n",
    "                if i.tag == 'JudgmentText':\n",
    "                    for legis in i:\n",
    "                        if legis.tag == 'I':\n",
    "                            for para in legis:\n",
    "                                if para.text is None:\n",
    "                                    continue\n",
    "                                flag = 0\n",
    "                                s = \"\"\n",
    "                                for sent in tokenizer.tokenize(para.text):\n",
    "                                    s_lower = sent.lower()\n",
    "                                    if ' vs' in s_lower or ' v. ' in sent or ' v ' in sent:\n",
    "                                        flag = 1\n",
    "                                        s += sent + \" \"\n",
    "                                if flag:\n",
    "                                    s_lower = s.lower()\n",
    "                                    if 'overruled' in s_lower:\n",
    "                                        out.write(\"Precedent Overruled : \")\n",
    "                                    elif 'distinguish' in s_lower:\n",
    "                                        out.write(\"Precedent Distinguished : \")\n",
    "                                    elif sia.polarity_scores(s)[\"compound\"] > 0:\n",
    "                                        out.write(\"Precedent Relied : \")\n",
    "                                    else:\n",
    "                                        out.write(\"Precedent Referred : \")\n",
    "                                    out.write(s.strip() + '\\n\\n')\n",
    "\n",
    "                        elif legis.tag == 'P':\n",
    "                            if legis.text is None:\n",
    "                                continue\n",
    "                            flag = 0\n",
    "                            s = \"\"\n",
    "                            for sent in tokenizer.tokenize(legis.text):\n",
    "                                s_lower = sent.lower()\n",
    "                                if ' vs' in s_lower or ' v. ' in sent or ' v ' in sent:\n",
    "                                    flag = 1\n",
    "                                    s += sent + \" \"\n",
    "                            if flag:\n",
    "                                s_lower = s.lower()\n",
    "                                if 'overruled' in s_lower:\n",
    "                                    out.write(\"Precedent Overruled : \")\n",
    "                                elif 'distinguish' in s_lower:\n",
    "                                    out.write(\"Precedent Distinguished : \")\n",
    "                                elif sia.polarity_scores(s)[\"compound\"] > 0:\n",
    "                                    out.write(\"Precedent Relied : \")\n",
    "                                else:\n",
    "                                    out.write(\"Precedent Referred : \")\n",
    "                                out.write(s.strip() + '\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c078b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To differentiate between capital V and small v\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "with open( 'Precedents output.txt', 'w') as out:\n",
    "    directories = ['Writ Petition', 'Civil Appeal']\n",
    "    for directory in directories:\n",
    "        for filename in os.listdir(directory):\n",
    "            f = os.path.join(directory, filename)\n",
    "            mytree = ET.ElementTree(file= f)\n",
    "            myroot = mytree.getroot()\n",
    "\n",
    "            for i in myroot:\n",
    "                if(i.tag=='JudgmentText'):\n",
    "                    for legis in i:\n",
    "                        # out.write(\"Inside Judgement Text : \" + legis.tag + '\\n')\n",
    "                        if(legis.tag=='I'):\n",
    "                            for para in legis:\n",
    "                                if para.text is None:\n",
    "                                    continue\n",
    "                                s1 = para.text.lower()\n",
    "                                s2 = para.text\n",
    "                                if 'overrule' in s1: # or (' v ' in s1 and ' v ' not in s2): \n",
    "                                    out.write(directory + ' : ' + filename  + ' : ' + \"Precedent : \" + para.text + '\\n\\n')\n",
    "\n",
    "                        elif(legis.tag=='P'):\n",
    "\n",
    "                            if legis.text is None:\n",
    "                                continue\n",
    "\n",
    "                            \n",
    "                            s1 = legis.text.lower()\n",
    "                            s2 = legis.text\n",
    "                            if 'overrule' in s1: # or (' v ' in s1 and ' v ' not in s2): \n",
    "                                out.write(directory + ' : '  + filename + ' : '  + \"Precedent : \" + legis.text + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7681067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.0.1)\n",
      "La carpeta New cases no existe. Por favor, verifica la ruta.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Sentence wise classification of precedents in pdf format\n",
    "! pip install PyPDF2\n",
    "import os\n",
    "import PyPDF2\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# Asegúrate de tener descargado 'punkt' para tokenización\n",
    "nltk.download('punkt')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "directories = [r'New cases']  # Cambia a la ruta correcta si es necesario\n",
    "\n",
    "for directory in directories:\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"La carpeta {directory} no existe. Por favor, verifica la ruta.\")\n",
    "        continue\n",
    "\n",
    "    out_dir = directory + ' precedents classified output'\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)  # Crear carpeta salida si no existe\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.lower().endswith('.pdf'):\n",
    "            continue  # Ignorar archivos que no son PDF\n",
    "\n",
    "        pdf_path = os.path.join(directory, filename)\n",
    "        out_file = os.path.join(out_dir, filename.replace(\".pdf\", \" output.txt\"))\n",
    "\n",
    "        with open(pdf_path, 'rb') as pdfFileObj:  # 'rb' para archivos binarios PDF\n",
    "            pdfReader = PyPDF2.PdfReader(pdfFileObj)\n",
    "            NumPages = len(pdfReader.pages)\n",
    "\n",
    "            with open(out_file, 'w', encoding='utf-8') as out:\n",
    "                for i in range(NumPages):\n",
    "                    page = pdfReader.pages[i]\n",
    "                    Text = page.extract_text()\n",
    "                    if not Text:\n",
    "                        continue\n",
    "                    flag = 0\n",
    "                    s = \"\"\n",
    "                    for sent in sent_tokenize(Text):\n",
    "                        s_lower = sent.lower()\n",
    "                        if ' vs' in s_lower or ' v. ' in sent or ' v ' in sent:\n",
    "                            flag = 1\n",
    "                            s += sent + \" \"\n",
    "                    if flag:\n",
    "                        s_lower = s.lower()\n",
    "                        if 'overruled' in s_lower:\n",
    "                            out.write(\"Precedent Overruled : \")\n",
    "                        elif 'distinguish' in s_lower:\n",
    "                            out.write(\"Precedent Distinguished : \")\n",
    "                        elif sia.polarity_scores(s)[\"compound\"] > 0:\n",
    "                            out.write(\"Precedent Relied : \")\n",
    "                        else:\n",
    "                            out.write(\"Precedent Referred : \")\n",
    "                        out.write(s.strip() + '\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70f20bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence wise classification of precedents\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "directories = ['Annotated Data_with xml and original PDF']\n",
    "for directory in directories:\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.xml'):\n",
    "            f = os.path.join(directory, filename)\n",
    "            mytree = ET.parse(f)\n",
    "\n",
    "            out_dir = directory + ' precedents classified output'\n",
    "\n",
    "            # Crear carpeta si no existe\n",
    "            if not os.path.exists(out_dir):\n",
    "                os.makedirs(out_dir)\n",
    "\n",
    "            out_file = os.path.join(out_dir, filename.replace(\".xml\", \" output.txt\"))\n",
    "            with open(out_file, 'w', encoding='utf-8') as out:\n",
    "                for i in mytree.iter():\n",
    "                    if i.text is not None:\n",
    "                        out.write(i.text + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "acdd0a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence wise classification of precedents from txt file for 26 cases\n",
    "\n",
    "import os\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "directories = ['Twenty_six']\n",
    "for directory in directories:\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        out_dir = directory+' output'\n",
    "        out_file = os.path.join(out_dir, filename.replace(\".txt\", \" output.txt\"))\n",
    "        with open(f, 'r') as in_file:\n",
    "            with open( out_file, 'w') as out:\n",
    "                for s2 in in_file:\n",
    "                    # out.write(line + '\\n')\n",
    "                    flag = 0\n",
    "                    s1 = s2.lower()\n",
    "                    if ' vs' in s1 or ' v. ' in s2 or ' v ' in s2:\n",
    "                        flag=1\n",
    "                    if flag:\n",
    "                        if 'overruled' in s1.lower():\n",
    "                            out.write(\"Precedent Overruled : \")\n",
    "                        elif 'distinguish' in s1.lower():\n",
    "                            out.write(\"Precedent Distinguished : \")\n",
    "                        elif sia.polarity_scores(s2)[\"compound\"] > 0:\n",
    "                            out.write(\"Precedent Relied : \")\n",
    "                        else:\n",
    "                            out.write(\"Precedent Referred : \")\n",
    "                        out.write(s1 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30f1a121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Code to extract and classify precedents from any XML file with text within JudgementText\n",
    "import os\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "\n",
    "# Descargar recursos necesarios solo una vez\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Inicializar analizador de sentimiento\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Configurar tokenizer explícito para inglés, evita problemas con recursos no encontrados\n",
    "punkt_param = PunktParameters()\n",
    "tokenizer = PunktSentenceTokenizer(punkt_param)\n",
    "\n",
    "def extract_judgment_text(plain_text):\n",
    "    \"\"\"Extrae el texto dentro de las etiquetas <JudgmentText>...</JudgmentText> ignorando etiquetas HTML internas.\"\"\"\n",
    "    sub1 = '<JudgmentText>'\n",
    "    sub2 = '</JudgmentText>'\n",
    "    try:\n",
    "        idx1 = plain_text.index(sub1) + len(sub1)\n",
    "        idx2 = plain_text.index(sub2)\n",
    "    except ValueError:\n",
    "        return None  # No encontró etiquetas\n",
    "    \n",
    "    text = \"\"\n",
    "    flag = 1\n",
    "    for i in range(idx1, idx2):\n",
    "        char = plain_text[i]\n",
    "        if char == '<':\n",
    "            flag = 0\n",
    "        elif char == '>':\n",
    "            flag = 1\n",
    "        elif flag:\n",
    "            text += char\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def classify_precedent(text, out_file):\n",
    "    \"\"\"\n",
    "    Clasifica y escribe en el archivo de salida los precedentes detectados en el texto.\n",
    "    El texto se tokeniza en oraciones, se busca la presencia de patrones ' vs', ' v.', ' v '.\n",
    "    Luego clasifica según palabras clave y sentimiento.\n",
    "    \"\"\"\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    flag = 0\n",
    "    prnt_snt = \"\"\n",
    "\n",
    "    for sent in sentences:\n",
    "        s_lower = sent.lower()\n",
    "        if ' vs' in s_lower or ' v. ' in s_lower or ' v ' in s_lower:\n",
    "            flag = 1\n",
    "            prnt_snt = sent\n",
    "        elif flag:\n",
    "            prnt_snt += \" \" + sent\n",
    "            s_combined = prnt_snt.lower()\n",
    "\n",
    "            if 'overruled' in s_combined:\n",
    "                out_file.write(\"Precedent Overruled : \")\n",
    "            elif 'distinguish' in s_combined:\n",
    "                out_file.write(\"Precedent Distinguished : \")\n",
    "            elif sia.polarity_scores(prnt_snt)[\"compound\"] > 0:\n",
    "                out_file.write(\"Precedent Relied : \")\n",
    "            else:\n",
    "                out_file.write(\"Precedent Referred : \")\n",
    "            out_file.write(prnt_snt.strip() + '\\n\\n')\n",
    "            flag = 0\n",
    "            prnt_snt = \"\"\n",
    "\n",
    "# Ruta de directorios con archivos XML\n",
    "directories = ['Annotated Data_with xml and original PDF']\n",
    "\n",
    "for directory in directories:\n",
    "    out_dir = directory + ' output'\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.lower().endswith('.xml'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            out_path = os.path.join(out_dir, filename.replace(\".xml\", \" output.txt\"))\n",
    "\n",
    "            with open(file_path, 'r', encoding='utf-8') as file, \\\n",
    "                 open(out_path, 'w', encoding='utf-8') as out:\n",
    "\n",
    "                plain_text = file.read()\n",
    "                judgment_text = extract_judgment_text(plain_text)\n",
    "\n",
    "                if judgment_text is None:\n",
    "                    print(f\"Etiqueta <JudgmentText> no encontrada en {filename}\")\n",
    "                    continue\n",
    "\n",
    "                classify_precedent(judgment_text, out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "428ba691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ------------ --------------------------- 3.9/12.8 MB 23.5 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 23.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 10.7/12.8 MB 19.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.6/12.8 MB 15.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 15.4 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "818d5599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.11.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (80.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\andre\\appdata\\roaming\\python\\python311\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Code to extract and classify precedents from any XML file with text within JudgementText\n",
    "! pip install spacy\n",
    "# Instalar spaCy y modelo en Google Colab\n",
    "\n",
    "import os\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "# Descargar recursos necesarios de nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Cargar modelo spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "directories = ['Annotated Data_with xml and original PDF']\n",
    "for directory in directories:\n",
    "    out_dir = directory + ' output'\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.xml'):\n",
    "            f = os.path.join(directory, filename)\n",
    "            out_file = os.path.join(out_dir, filename.replace(\".xml\", \" output.txt\"))\n",
    "            \n",
    "            with open(f, 'r', encoding='utf-8') as file, open(out_file, 'w', encoding='utf-8') as out:\n",
    "                plain_text = file.read()\n",
    "                sub1 = '<JudgmentText>'\n",
    "                sub2 = '</JudgmentText>'\n",
    "                \n",
    "                try:\n",
    "                    idx1 = plain_text.index(sub1) + len(sub1)\n",
    "                    idx2 = plain_text.index(sub2)\n",
    "                except ValueError:\n",
    "                    print(f\"Etiqueta <JudgmentText> no encontrada en {filename}\")\n",
    "                    continue\n",
    "                \n",
    "                # Extraer texto dentro de <JudgmentText> ignorando etiquetas HTML/XML\n",
    "                text = \"\"\n",
    "                flag = 1\n",
    "                for i in range(idx1, idx2):\n",
    "                    char = plain_text[i]\n",
    "                    if char == '<':\n",
    "                        flag = 0\n",
    "                    elif char == '>':\n",
    "                        flag = 1\n",
    "                    elif flag:\n",
    "                        text += char\n",
    "                text = \" \".join(text.split())\n",
    "\n",
    "                # Procesar texto con spaCy para obtener oraciones\n",
    "                doc = nlp(text)\n",
    "                sentences = list(doc.sents)\n",
    "\n",
    "                flag = 0\n",
    "                prnt_snt = \"\"\n",
    "\n",
    "                for sent in sentences:\n",
    "                    s1 = sent.text.lower()\n",
    "                    if ' vs' in s1 or ' v. ' in s1 or ' v ' in s1:\n",
    "                        flag = 1\n",
    "                        prnt_snt = sent.text\n",
    "                    elif flag:\n",
    "                        prnt_snt += \" \" + sent.text\n",
    "                        s1 = prnt_snt.lower()\n",
    "                        if 'overruled' in s1:\n",
    "                            out.write(\"Precedent Overruled : \")\n",
    "                        elif 'distinguish' in s1:\n",
    "                            out.write(\"Precedent Distinguished : \")\n",
    "                        elif sia.polarity_scores(prnt_snt)[\"compound\"] > 0:\n",
    "                            out.write(\"Precedent Relied : \")\n",
    "                        else:\n",
    "                            out.write(\"Precedent Referred : \")\n",
    "                        \n",
    "                        out.write(prnt_snt + '\\n\\n')\n",
    "                        flag = 0\n",
    "                        prnt_snt = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e6e58c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Solo necesario una vez\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import spacy\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Inicializar spaCy y el analizador de sentimientos\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Asegúrate de ejecutar: python -m spacy download en_core_web_sm\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Directorio con archivos .json\n",
    "directories = ['new_case_json_100']\n",
    "\n",
    "for directory in directories:\n",
    "    out_dir = directory + '_output'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.lower().endswith('.json'):\n",
    "            continue\n",
    "\n",
    "        input_path = os.path.join(directory, filename)\n",
    "        output_path = os.path.join(out_dir, filename.replace(\".json\", \" output.txt\"))\n",
    "\n",
    "        with open(input_path, 'r', encoding='utf-8') as file, \\\n",
    "             open(output_path, 'w', encoding='utf-8') as out:\n",
    "\n",
    "            try:\n",
    "                data = json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error al cargar {filename}, archivo JSON inválido.\")\n",
    "                continue\n",
    "\n",
    "            # Concatenar texto del campo JudgmentText\n",
    "            text = \"\"\n",
    "            for line in data.get('JudgmentText', []):\n",
    "                if isinstance(line, str):\n",
    "                    text += line\n",
    "                elif isinstance(line, dict) and 'I' in line:\n",
    "                    if isinstance(line['I'], str):\n",
    "                        text += line['I']\n",
    "                    elif isinstance(line['I'], list):\n",
    "                        text += ''.join(map(str, line['I']))\n",
    "\n",
    "            text = \" \".join(text.split())  # Limpiar espacios\n",
    "\n",
    "            # Tokenización de oraciones con spaCy (más confiable que nltk)\n",
    "            doc = nlp(text)\n",
    "            sentences = list(doc.sents)\n",
    "\n",
    "            flag = 0\n",
    "            prnt_snt = \"\"\n",
    "\n",
    "            for sent in sentences:\n",
    "                s_lower = sent.text.lower()\n",
    "\n",
    "                if ' vs' in s_lower or ' v. ' in s_lower or ' v ' in s_lower:\n",
    "                    flag = 1\n",
    "                    prnt_snt = sent.text\n",
    "                elif flag:\n",
    "                    prnt_snt += \" \" + sent.text\n",
    "                    s_combined = prnt_snt.lower()\n",
    "\n",
    "                    if 'overruled' in s_combined:\n",
    "                        out.write(\"Precedent Overruled : \")\n",
    "                    elif 'distinguish' in s_combined:\n",
    "                        out.write(\"Precedent Distinguished : \")\n",
    "                    elif sia.polarity_scores(prnt_snt)[\"compound\"] > 0:\n",
    "                        out.write(\"Precedent Relied : \")\n",
    "                    else:\n",
    "                        out.write(\"Precedent Referred : \")\n",
    "\n",
    "                    out.write(prnt_snt.strip() + \"\\n\\n\")\n",
    "                    flag = 0\n",
    "                    prnt_snt = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e30ec379",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check for precedents\n",
    "import json\n",
    "\n",
    "with open('jsonformatter.txt', 'r', encoding='utf-8') as f:\n",
    "    with open(\"Precedents output.txt\", 'w', encoding='utf-8') as out:\n",
    "        data = json.load(f)\n",
    "\n",
    "        for case in data:\n",
    "            out.write('Case id : ' + str(case['id']) + '\\n')\n",
    "            annotation = case['annotations'][0]\n",
    "            for key in annotation['result']:\n",
    "                if key['value']['labels'][0] == 'STA':\n",
    "                    out.write('STA : ' + key['value']['text'] + '\\n\\n')\n",
    "            out.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6c54dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Inicializar spaCy y SentimentIntensityAnalyzer\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Ejecuta: python -m spacy download en_core_web_sm si no lo tienes\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "directories = ['new_case_json_100']\n",
    "\n",
    "for directory in directories:\n",
    "    out_dir = directory + '_output'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.lower().endswith('.json'):\n",
    "            continue\n",
    "\n",
    "        input_path = os.path.join(directory, filename)\n",
    "        output_path = os.path.join(out_dir, filename.replace(\".json\", \" output.json\"))\n",
    "\n",
    "        with open(input_path, 'r', encoding='utf-8') as file, \\\n",
    "             open(output_path, 'w', encoding='utf-8') as out:\n",
    "\n",
    "            try:\n",
    "                data = json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error al cargar {filename}, archivo JSON inválido.\")\n",
    "                continue\n",
    "\n",
    "            # Concatenar texto de JudgmentText\n",
    "            text = \"\"\n",
    "            for line in data.get('JudgmentText', []):\n",
    "                if isinstance(line, str):\n",
    "                    text += line + \" \"\n",
    "                elif isinstance(line, dict) and 'I' in line:\n",
    "                    if isinstance(line['I'], str):\n",
    "                        text += line['I'] + \" \"\n",
    "                    elif isinstance(line['I'], list):\n",
    "                        text += ' '.join(map(str, line['I'])) + \" \"\n",
    "\n",
    "            text = \" \".join(text.split())  # Limpiar espacios extra\n",
    "\n",
    "            # Tokenizar oraciones con spaCy\n",
    "            doc = nlp(text)\n",
    "            sentences = list(doc.sents)\n",
    "\n",
    "            output = {}\n",
    "            flag = 0\n",
    "            prnt_snt = \"\"\n",
    "\n",
    "            for sent in sentences:\n",
    "                sent_text = sent.text.strip()\n",
    "                sent_lower = sent_text.lower()\n",
    "\n",
    "                if ' vs' in sent_lower or ' v. ' in sent_lower or ' v ' in sent_lower:\n",
    "                    flag = 1\n",
    "                    prnt_snt = sent_text\n",
    "                elif flag:\n",
    "                    prnt_snt += \" \" + sent_text\n",
    "                    prnt_lower = prnt_snt.lower()\n",
    "\n",
    "                    if 'overruled' in prnt_lower:\n",
    "                        output[prnt_snt] = \"PRE_OVER\"\n",
    "                    elif 'distinguish' in prnt_lower:\n",
    "                        output[prnt_snt] = \"PRE_DIST\"\n",
    "                    elif sia.polarity_scores(prnt_snt)[\"compound\"] > 0:\n",
    "                        output[prnt_snt] = \"PRE_REL\"\n",
    "                    else:\n",
    "                        output[prnt_snt] = \"PRE_REF\"\n",
    "\n",
    "                    flag = 0\n",
    "                    prnt_snt = \"\"\n",
    "\n",
    "            # Guardar salida como JSON formateado\n",
    "            json_output = json.dumps(output, indent=4, ensure_ascii=False)\n",
    "            out.write(json_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0439ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to extract and classify parts of Judgements into XML meta tags\n",
    "import os\n",
    "\n",
    "directories = ['abc']\n",
    "for directory in directories:\n",
    "    out_dir = directory+'_output'\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        out_file = os.path.join(out_dir, filename.replace(\".txt\", \"_output.txt\"))\n",
    "        with open(f, 'r') as file:\n",
    "            with open( out_file, 'w') as out:\n",
    "                lines = file.readlines()\n",
    "                state=0\n",
    "                for line in lines:\n",
    "                    line1 = line.lower()\n",
    "                    if state == 0 and 'reportable' in line1:\n",
    "                        state = 1\n",
    "                        continue\n",
    "                    if state == 1 and 'court' in line1:\n",
    "                        state = 2\n",
    "                        out.write('Court : ' + line[line1.index('the') + 3:].strip() + '\\n')\n",
    "                        continue\n",
    "                    if state == 2 and 'jurisdiction' in line1:\n",
    "                        state = 3\n",
    "                        # out.write('        ' + line.strip() + '\\n')\n",
    "                        continue\n",
    "                    if (state >= 3 and state < 8) and ('appeal' in line1 or 'petition' in line1) and 'no.' in line1:\n",
    "                        state = max(4, state)\n",
    "                        out.write(\"Case No. : \" + line.strip() + '\\n')\n",
    "                        continue\n",
    "                    if state == 4 and '.appellant' in line1:\n",
    "                        state = 5\n",
    "                        prnt_snt = \"Petitioner : \"\n",
    "                        for word in line.split():\n",
    "                            if 'appellant' in word.lower():\n",
    "                                continue\n",
    "                            prnt_snt += (word + ' ')\n",
    "                        prnt_snt += '\\n'\n",
    "                        out.write(prnt_snt)\n",
    "                        continue\n",
    "                    if state == 5 and 'respondent' in line1:\n",
    "                        state = 6\n",
    "                        prnt_snt = \"Respondent : \"\n",
    "                        for word in line.split():\n",
    "                            if 'respondent' in word.lower():\n",
    "                                continue\n",
    "                            prnt_snt += (word + ' ')\n",
    "                        prnt_snt += '\\n'\n",
    "                        out.write(prnt_snt)\n",
    "                        continue\n",
    "                    if state == 6 and 'judgment' in line1.replace(\" \", \"\"):\n",
    "                        state = 7\n",
    "                        continue\n",
    "                    if state == 7 and len(line) >= 2:\n",
    "                        state = 8\n",
    "                        out.write(\"Judge Name : \" + line.split(',')[0].strip() + '\\n')\n",
    "                        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ca92bf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando directorio: abc_output_ujp\n",
      "Procesando archivo: abc\\01 03 2021 SC 10355 OF 2018.txt\n",
      "Archivo de salida: abc_output_ujp\\01 03 2021 SC 10355 OF 2018_output.txt\n",
      "Procesando archivo: abc\\01 03 2021 SC 142 OF 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\01 03 2021 SC 142 OF 2021_output.txt\n",
      "Procesando archivo: abc\\01 03 2021 SC 167 OF 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\01 03 2021 SC 167 OF 2021_output.txt\n",
      "Procesando archivo: abc\\01 03 2021 SC 202 OF 2012.txt\n",
      "Archivo de salida: abc_output_ujp\\01 03 2021 SC 202 OF 2012_output.txt\n",
      "Procesando archivo: abc\\01 03 2021 SC 4230-4234 OF 2020.txt\n",
      "Archivo de salida: abc_output_ujp\\01 03 2021 SC 4230-4234 OF 2020_output.txt\n",
      "Procesando archivo: abc\\01 03 2021 SC 580 OF 2018.txt\n",
      "Archivo de salida: abc_output_ujp\\01 03 2021 SC 580 OF 2018_output.txt\n",
      "Procesando archivo: abc\\01 03 2021 SC 738-739 OF 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\01 03 2021 SC 738-739 OF 2021_output.txt\n",
      "Procesando archivo: abc\\01 03 2021 SC 946 OF 2016.txt\n",
      "Archivo de salida: abc_output_ujp\\01 03 2021 SC 946 OF 2016_output.txt\n",
      "Procesando archivo: abc\\02 03 2021 SC 1348 OF 2013.txt\n",
      "Archivo de salida: abc_output_ujp\\02 03 2021 SC 1348 OF 2013_output.txt\n",
      "Procesando archivo: abc\\02 03 2021 SC 791 OF 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\02 03 2021 SC 791 OF 2021_output.txt\n",
      "Procesando archivo: abc\\03 03 2021 SC 639-640 of 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\03 03 2021 SC 639-640 of 2021_output.txt\n",
      "Procesando archivo: abc\\04 03 2021 SC 267 OF 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\04 03 2021 SC 267 OF 2021_output.txt\n",
      "Procesando archivo: abc\\04 03 2021 SC 3592-3593 OF 2020.txt\n",
      "Archivo de salida: abc_output_ujp\\04 03 2021 SC 3592-3593 OF 2020_output.txt\n",
      "Procesando archivo: abc\\04 03 2021 SC 810 OF 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\04 03 2021 SC 810 OF 2021_output.txt\n",
      "Procesando archivo: abc\\04 03 2021 SC 980 OF 2019.txt\n",
      "Archivo de salida: abc_output_ujp\\04 03 2021 SC 980 OF 2019_output.txt\n",
      "Procesando archivo: abc\\05 03 2021 SC 3745-3754 OF 2020.txt\n",
      "Archivo de salida: abc_output_ujp\\05 03 2021 SC 3745-3754 OF 2020_output.txt\n",
      "Procesando archivo: abc\\05 03 2021 SC 495 of 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\05 03 2021 SC 495 of 2021_output.txt\n",
      "Procesando archivo: abc\\05 03 2021 SC 814-815 Of 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\05 03 2021 SC 814-815 Of 2021_output.txt\n",
      "Procesando archivo: abc\\05 03 2021 SC 821 of 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\05 03 2021 SC 821 of 2021_output.txt\n",
      "Procesando archivo: abc\\08 03 2021 SC 258 OF 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\08 03 2021 SC 258 OF 2021_output.txt\n",
      "Procesando archivo: abc\\08 03 2021 SC 283   OF 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\08 03 2021 SC 283   OF 2021_output.txt\n",
      "Procesando archivo: abc\\08 03 2021 SC 3 of 2020.txt\n",
      "Archivo de salida: abc_output_ujp\\08 03 2021 SC 3 of 2020_output.txt\n",
      "Procesando archivo: abc\\08 03 2021 SC 37 OF 2009.txt\n",
      "Archivo de salida: abc_output_ujp\\08 03 2021 SC 37 OF 2009_output.txt\n",
      "Procesando archivo: abc\\08 03 2021 SC 443 OF 2020.txt\n",
      "Archivo de salida: abc_output_ujp\\08 03 2021 SC 443 OF 2020_output.txt\n",
      "Procesando archivo: abc\\08 03 2021 SC 825 OF 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\08 03 2021 SC 825 OF 2021_output.txt\n",
      "Procesando archivo: abc\\09 03 2021 SC 1827 OF 2018.txt\n",
      "Archivo de salida: abc_output_ujp\\09 03 2021 SC 1827 OF 2018_output.txt\n",
      "Procesando archivo: abc\\09 03 2021 SC 292 OF 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\09 03 2021 SC 292 OF 2021_output.txt\n",
      "Procesando archivo: abc\\09 03 2021 SC 413 OF 2019.txt\n",
      "Archivo de salida: abc_output_ujp\\09 03 2021 SC 413 OF 2019_output.txt\n",
      "Procesando archivo: abc\\10 03 2021 SC 2943-2944 OF 2020.txt\n",
      "Archivo de salida: abc_output_ujp\\10 03 2021 SC 2943-2944 OF 2020_output.txt\n",
      "Procesando archivo: abc\\10 03 2021 SC 296 OF 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\10 03 2021 SC 296 OF 2021_output.txt\n",
      "Procesando archivo: abc\\10 03 2021 SC 843-844 OF 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\10 03 2021 SC 843-844 OF 2021_output.txt\n",
      "Procesando archivo: abc\\10 03 2021 SC 845 OF 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\10 03 2021 SC 845 OF 2021_output.txt\n",
      "Procesando archivo: abc\\12 03 2021 SC 5102 of 2020.txt\n",
      "Archivo de salida: abc_output_ujp\\12 03 2021 SC 5102 of 2020_output.txt\n",
      "Procesando archivo: abc\\12 03 2021 SC 881 OF 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\12 03 2021 SC 881 OF 2021_output.txt\n",
      "Procesando archivo: abc\\15 03 2021 SC 298-299 OF 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\15 03 2021 SC 298-299 OF 2021_output.txt\n",
      "Procesando archivo: abc\\15 03 2021 SC 320 OF 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\15 03 2021 SC 320 OF 2021_output.txt\n",
      "Procesando archivo: abc\\17 03 2021 SC 25 OF 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\17 03 2021 SC 25 OF 2021_output.txt\n",
      "Procesando archivo: abc\\19 03 2021 SC 336 OF 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\19 03 2021 SC 336 OF 2021_output.txt\n",
      "Procesando archivo: abc\\19 03 2021 SC 995 OF 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\19 03 2021 SC 995 OF 2021_output.txt\n",
      "Procesando archivo: abc\\23 03 2021 SC 121 OF 2019.txt\n",
      "Archivo de salida: abc_output_ujp\\23 03 2021 SC 121 OF 2019_output.txt\n",
      "Procesando archivo: abc\\23 03 2021 SC 337 OF 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\23 03 2021 SC 337 OF 2021_output.txt\n",
      "Procesando archivo: abc\\23 03 2021 SC 476 OF 2020.txt\n",
      "Archivo de salida: abc_output_ujp\\23 03 2021 SC 476 OF 2020_output.txt\n",
      "Procesando archivo: abc\\23 03 2021 SC 49 OF 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\23 03 2021 SC 49 OF 2021_output.txt\n",
      "Procesando archivo: abc\\24 03 2021 SC 3395 OF 2020.txt\n",
      "Archivo de salida: abc_output_ujp\\24 03 2021 SC 3395 OF 2020_output.txt\n",
      "Procesando archivo: abc\\24 03 2021 SC 363 OF 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\24 03 2021 SC 363 OF 2021_output.txt\n",
      "Procesando archivo: abc\\25 03 2021 SC 1007 OF 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\25 03 2021 SC 1007 OF 2021_output.txt\n",
      "Procesando archivo: abc\\25 03 2021 SC 1109 of 2020.txt\n",
      "Archivo de salida: abc_output_ujp\\25 03 2021 SC 1109 of 2020_output.txt\n",
      "Procesando archivo: abc\\25 03 2021 SC 141 of 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\25 03 2021 SC 141 of 2021_output.txt\n",
      "Procesando archivo: abc\\25 03 2021 SC 25047 OF 2018.txt\n",
      "Archivo de salida: abc_output_ujp\\25 03 2021 SC 25047 OF 2018_output.txt\n",
      "Procesando archivo: abc\\25 03 2021 SC 554-000557 of 2021.txt\n",
      "Archivo de salida: abc_output_ujp\\25 03 2021 SC 554-000557 of 2021_output.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directories = [\"abc\"]\n",
    "months = {\n",
    "    \"january\":\"01\", \"february\":\"02\", \"march\":\"03\", \"april\":\"04\",\n",
    "    \"may\":\"05\", \"june\":\"06\", \"july\":\"07\", \"august\":\"08\",\n",
    "    \"september\":\"09\", \"october\":\"10\", \"november\":\"11\", \"december\":\"12\"\n",
    "}\n",
    "\n",
    "for directory in directories:\n",
    "    out_dir = directory + '_output_ujp'\n",
    "\n",
    "    # Crear el directorio de salida si no existe\n",
    "    if not os.path.exists(out_dir):\n",
    "        print(f\"Creando directorio: {out_dir}\")\n",
    "        os.makedirs(out_dir)\n",
    "    else:\n",
    "        print(f\"Directorio ya existe: {out_dir}\")\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        out_file = os.path.join(out_dir, filename.replace(\".txt\", \"_output.txt\"))\n",
    "        print(f\"Procesando archivo: {f}\")\n",
    "        print(f\"Archivo de salida: {out_file}\")\n",
    "\n",
    "        check = {\n",
    "            \"Reportable\": False, \"Court\": False, \"Case no\": False,\n",
    "            \"Petitioner\": False, \"Respondent\": False, \"Judgement\": False,\n",
    "            \"Judge name\": False, \"Date\": False, \"Bench\": False\n",
    "        }\n",
    "\n",
    "        if f.lower().endswith(\"txt\"):\n",
    "            try:\n",
    "                with open(f, 'r', encoding='utf-8') as file, \\\n",
    "                     open(out_file, 'w', encoding='utf-8') as out:\n",
    "\n",
    "                    lines = file.readlines()\n",
    "                    lines = [line.strip() for line in lines]\n",
    "\n",
    "                    bench = []\n",
    "                    while \"\" in lines:\n",
    "                        lines.remove(\"\")\n",
    "\n",
    "                    for line in lines:\n",
    "                        line1 = line.lower()\n",
    "\n",
    "                        if not check[\"Reportable\"] and 'reportable' in line1:\n",
    "                            check[\"Reportable\"] = True\n",
    "\n",
    "                        if check[\"Reportable\"] and not check[\"Court\"] and 'court' in line1:\n",
    "                            check[\"Court\"] = True\n",
    "                            out.write('Court : ' + line.strip() + '\\n')\n",
    "\n",
    "                        if check[\"Reportable\"] and check[\"Court\"] and 'jurisdiction' in line1:\n",
    "                            out.write('        ' + line.strip() + '\\n')\n",
    "\n",
    "                        if not check[\"Case no\"] and 'appeal' in line1:\n",
    "                            check[\"Case no\"] = True\n",
    "                            out.write(\"Case No. : \" + line.strip() + '\\n')\n",
    "\n",
    "                        if not check[\"Petitioner\"] and '.appellant' in line1:\n",
    "                            check[\"Petitioner\"] = True\n",
    "                            prnt_snt = \"Petitioner : \"\n",
    "                            for word in line.split():\n",
    "                                if 'appellant' in word.lower():\n",
    "                                    continue\n",
    "                                prnt_snt += (word + ' ')\n",
    "                            prnt_snt += '\\n'\n",
    "                            out.write(prnt_snt)\n",
    "\n",
    "                        if not check[\"Respondent\"] and 'respondent' in line1:\n",
    "                            check[\"Respondent\"] = True\n",
    "                            prnt_snt = \"Respondent : \"\n",
    "                            for word in line.split():\n",
    "                                if 'respondent' in word.lower():\n",
    "                                    continue\n",
    "                                prnt_snt += (word + ' ')\n",
    "                            prnt_snt += '\\n'\n",
    "                            out.write(prnt_snt)\n",
    "\n",
    "                        if not check[\"Judgement\"] and 'judgment' in line1.replace(\" \", \"\"):\n",
    "                            check[\"Judgement\"] = True\n",
    "\n",
    "                        if check[\"Judgement\"] and not check[\"Judge name\"] and len(line) >= 2:\n",
    "                            check[\"Judge name\"] = True\n",
    "                            out.write(\"Judge Name : \" + line + \"\\n\")\n",
    "\n",
    "                        if not check[\"Bench\"]:\n",
    "                            if \"[\" in line1 and \"]\" in line1:\n",
    "                                a = line1.find(\"[\")\n",
    "                                b = line1.find(\"]\")\n",
    "                                if \".\" in line1[a+1:b] and \"(\" not in line1[a+1:b]:\n",
    "                                    bench.append(line[a+1:b])\n",
    "                            if line == lines[-1]:\n",
    "                                check[\"Bench\"] = True\n",
    "                                out.write(\"Bench : \" + str(bench) + \"\\n\")\n",
    "\n",
    "                        if not check[\"Date\"] and line == lines[-1]:\n",
    "                            check[\"Date\"] = True\n",
    "                            temp = line1.split(\" \")\n",
    "                            if len(temp) < 3:\n",
    "                                continue\n",
    "                            month = temp[0]\n",
    "                            date = temp[1].replace(\",\", \"\")\n",
    "                            year = temp[2].replace(\".\", \"\")\n",
    "                            if month in months:\n",
    "                                month_num = months[month]\n",
    "                                final_date = f\"{date}/{month_num}/{year}\\n\"\n",
    "                                out.write(\"Judgement Date : \" + final_date)\n",
    "            except Exception as e:\n",
    "                print(f\"Error al procesar {f}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "276abe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check for precedents\n",
    "import json\n",
    "\n",
    "with open('jsonformatter.txt', 'r', encoding='utf-8') as f:\n",
    "    with open(\"Precedents output.txt\", 'w', encoding='utf-8') as out:\n",
    "        data = json.load(f)\n",
    "\n",
    "        for case in data:\n",
    "            out.write('Case id : ' + str(case['id']) + '\\n')\n",
    "            annotation = case['annotations'][0]\n",
    "            for key in annotation['result']:\n",
    "                if key['value']['labels'][0] == 'STA':\n",
    "                    out.write('STA : ' + key['value']['text'] + '\\n\\n')\n",
    "            out.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c1b5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
